# 2-1. 선형회귀

### 1. 선형회귀의 정의, 종류

선형회귀(linear regression) 란, 이름처럼 선형의 예측선을 그려서 다음의 들어올 값을 예측선에 따라서 예측하는 것이다.

간단하게 ```학생들의 성적이 다르다.```를, 
```학생들의 성적이 [ ]에 따라 다르다.```라고도 이야기할 수 있는데, 
여기서 [ ] 안에 들어가는 내용을 정보라고 한다. 
그래서 정보 중에서 공부시간을 예로 든다면, ```학생들의 성적이 [공부시간]에 따라 다르다.```라고 할 수 있을 것이다. 그러면, 공부시간에 따라서 성적이 다르게 나오므로, 학생들의 공부시간을 가지고 성적도 예측할 수 있을 것이다.

이것을 함수로 나타낸다면, 우리가 아는 일차함수로도 표현할 수 있을 것이다. 어떻게 하냐면, 성적과 같은 결과 값을 변하게 하는 ‘정보’를 x로 하고, 이 **정보(x)**에 따라서 변하는 성적과 같은 **결과를 y**라고 할 수 있다.

여기서 이 x와 y를 수학적인 표현으로 바꾸면, x는 독립적으로 변하는 **독립변수(x)** 라고 하며, y는 독립변수에 따라서 종속되어서 변하므로, **종속변수(y)** 라 한다.

여기서 선형회귀는 독립변수(x)로 종속변수(y)를 예측하는 작업이다. 
그리고 독립변수(x)가 하나뿐이라면 **단순 선형 회귀(simple linear regreession)**, 반대로 여러 개라면 **다중 선형 회귀(multiple linear regression)** 이라고 한다.

---

단순 선형 회귀는 우리가 쉽게 아는 y = ax + b 라는 일차함수의 그래프로 그려지는 직선이다.  
이 선으로, x값에 따른 y값에 대한 오차가 적은(정확한) 일직선을 그리는 것이 예측의 관건이 된다.   
이 y = ax + b 의 정확한 일차함수 직선을 만들려면, **“최소제곱법”** 을 이용하면 되는데, 이것이 회귀분석의 표준 방식이다. 

### 2. 최소 제곱법

최소 제곱법(method of least squares)은 평균을 사용해서 가장 오차가 적은 기울기(a)와 절편(b)을 구해내는 공식이다. 

![image](https://user-images.githubusercontent.com/48408417/86512304-eacef100-be3b-11ea-9d08-8feb6655c9ea.png) 

최소 제곱법의 공식은 위와 같이 어려운 공식으로 나타내는데, 쉽게 말하면,
(x-x평균)(y-y평균)의 모든 합을 (x-x평균)의 제곱의 모든 합으로 나눈 값이 기울기(a)다. 

![image](https://user-images.githubusercontent.com/48408417/86512322-0cc87380-be3c-11ea-86b1-e71be52ef5f8.png)


절편(b)도 쉽게 풀면, y의 평균 - (x의 평균 * a)와 같다.

근데 이 방법은 하나의 x에서 입력을 쉽게 처리할 때는 문제가 없지만, 입력이 여러 개가 되다보면, 오차가 생길 수 있다.  
그래서 이 오차를 평가하는 알고리즘으로, 예측선(y = ax + b)을 고쳐주어야 하는데, 이 알고리즘이   
**“평균 제곱근 오차”, “평균 제곱 오차”** 다.

